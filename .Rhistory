library(tidyverse)
library(dplyr)
library(RedditExtractoR)
knitr::opts_chunk$set(echo = T, results = "hide")
#install.packages("revealjs")
library(tidyverse)
library(dplyr)
library(RedditExtractoR)
#Theme: “default”, “dark”, “simple”, “sky”, “beige”, “serif”, “solarized”, “blood”, “moon”, “night”, “black”, “league”, “white”
reddit_links <- reddit_urls(
search_terms   = "cute_cats",
page_threshold = 1
)
source('~/.active-rstudio-document', echo=TRUE)
View(reddit_thread)
reddit_thread <- reddit_content(reddit_links$URL[1])
str(reddit_thread)
str(reddit_thread)
str(reddit_thread)
str(reddit_thread)
library(tidyverse)
library(dplyr)
library(RedditExtractoR)
reddit_links <- reddit_urls(search_terms   = "cute_cats",page_threshold = 1)
str(reddit_links)
str(reddit_links)
reddit_data = get_reddit(search_terms = "science",subreddit = "science",cn_threshold=10)
source('~/Desktop/CIS8393 - Using R/Project/Reddit.R')
df <- data.frame(reddit_data)
df
View(df)
View(df)
install.packages(c("repurrrsive", "jsonlite", "tidyverse"))
library(repurrrsive)
library(lubridate)
library(stringr)
library(tidyverse)
library(lubridate)
library(repurrrsive)
today()
now()
dmy("31-Jan-2017")
mdy("January 31st, 2017")
mdy("January 31, 2017")
mdy("January 31st 2017")
mdy("Jan 31 2017")
as_date(now())
make_date(2018,1)
make_date(2018,1,21)
make_date(2018,13,21)
age
bdate = ymd(19970104)
age <- today() - bdate
age
class("age")
class(age)
as.duration(age)
d_age <- as.duration(age)
class(d_age)
d_age <- as.duration(age)
d_age
age
d_age
bdate = ymd(19970401)
age <- today() - bdate
age
d_age <- as.duration(age)
d_age
one_pm + ddays(1)
#PERIOD
(one_pm <- ymd_hms("2016-03-12 13:00:00", tz = "America/New_York"))
one_pm + ddays(1)
one_pm + days(1)
library(repurrrsive)
library(lubridate)
library(stringr)
library(tidyverse)
d_age
bdate = ymd(19970401)
d_age <- as.duration(today() - bdate)
d_age
days = today() - f_class
#How many days it has been since the first date of this course
f_class = ymd(20200108)
days = today() - f_class
days
#What is the date that is 7 weeks after July 1, 2010?
initialData <- ymd("2010-07-01", tz = "America/New_York"))
#What is the date that is 7 weeks after July 1, 2010?
(initialData <- ymd("2010-07-01", tz = "America/New_York"))
initialData + ddays(49) #7 weeks = 49 days
library(datasets)
source('~/Desktop/CIS8393 - Using R/Notes with R/Lecture 2/InClass3.R')
source('~/Desktop/CIS8393 - Using R/Notes with R/Lecture 2/InClass3.R')
source('~/Desktop/CIS8393 - Using R/Notes with R/Lecture 2/InClass3.R')
source('~/Desktop/CIS8393 - Using R/Notes with R/Lecture 2/InClass3.R')
repurrrsive::gh_users
repurrrsive::gh_users %>%
map(~.$created_at)
#Extract year from the created_at information from each of the individuals in
repurrrsive::gh_users %>%
map(~year(.$created_at))
repurrrsive::gh_users %>%
map(~as.duration(date(.$updated_at) - .$created_at))
repurrrsive::gh_users %>%
map(~as.duration(as.Date(.$updated_at) - .$created_at))
repurrrsive::gh_users %>%
map(~as.duration(ymd_hms(.$updated_at) - .$created_at))
#Get the duration between updated_at and created_at
repurrrsive::gh_users %>%
map(~as.duration(ymd_hms(.$updated_at) - ymd_hms(.$created_at)))
#Print CIty and State separately
x <- c("Atlanta, GA", "New York, NY", "Los Angeles, CA")
x =tr_to_upper(x)
x = str_to_upper(x)
x
x = tr_c(x, sep=",")
x = str_c(x, sep=",")
x
x = str_c(x, sep=", ")
x
state <- str_sub(x, -2, -1)
state
library(purrr)
x = str_split(x, sep=", ", simplify = T)
x = str_split(x, ", ", simplify = T)
x
x[,1]
x[,2]
city_state <- x[,1] %>%
str_c(., collapse = ", ") %>%
str_c("Cities: " ,.)
city_state
city <- x[,1] %>%
str_c(., collapse = ", ") %>%
str_c("Cities: " ,.)
city
state <- x[,2] %>%
str_c(., collapse = ", ") %>%
str_c("States: " ,.)
state
#Find all states with a spance in its name
state.name
#Find all states with a spance in its name
x = state.name
str_detect(x, " ")
str_locate_all(x, " ")
str_locate(x, " ")
str_detect(x, " ")
x[str_detect(x, " ")]
#Find all states with ss in its name
x[str_detect(x, "ss")]
#Find all state names that start with "South" and end with "na"
x[str_extract(x, "South+na")]
x[str_detect(x, "South+na")]
#Find all state names that start with "South" and end with "na"
x[str_detect(x, "[^South]...na$")]
#Find all state names that start with "South" and end with "na"
x[str_detect(x, "[^South]na$")]
#Find all state names that start with "South" and end with "na"
x[str_detect(x, "^South...na$")]
#Find all state names that start with "South" and end with "na"
x[str_detect(x, "^South+na$")]
#Find all state names that start with "South" and end with "na"
x[str_detect(x, "[^South]+na$")]
#Find all state names that start with "South" and end with "na"
x[str_detect(x, "[^South]")]
#Find all state names that start with "South" and end with "na"
x[str_detect(x, "^South")]
#Find all state names that start with "South" and end with "na"
x[str_detect(x, "^South[na$]")]
#Find all state names that start with "South" and end with "na"
x[str_detect(x, "^South[na]")]
#Find all state names that start with "South" and end with "na"
x[str_detect(x, "^South\\na")]
#Find all state names that start with "South" and end with "na"
x[str_detect(x, "^South,na$")]
#Find all state names that start with "South" and end with "na"
x[str_detect(x, "^South+na$")]
#Find all state names that start with "South" and end with "na"
x[str_detect(x, "^South | na$")]
x <- c("4044132000", "520-123-2000", "844.999.4500", "(400) 123-4567", "10/30/2017", "100,000,000")p
hone_number_regex_str = "your regex string goes here"
str_detect(x, phone_number_regex_str)
x <- c("4044132000", "520-123-2000", "844.999.4500", "(400) 123-4567", "10/30/2017", "100,000,000")p
hone_number_regex_str = "/^[+]?(\d{1,2})"
str_detect(x, phone_number_regex_str)
hone_number_regex_str = "(^| )[0-9.() -]{5,}( |$)"
str_detect(x, phone_number_regex_str)
phone_number_regex_str = "(^| )[0-9.() -]{5,}( |$)"
str_detect(x, phone_number_regex_str)
y <- c("4044132000", "520-123-2000", "844.999.4500", "(400) 123-4567", "10/30/2017", "100,000,000")
phone_number_regex_str = "(^| )[0-9.() -]{5,}( |$)"
str_detect(y, phone_number_regex_str)
regex_phonenumber = "\\d{3}\\[:punct:]?\\d{3}\\[:punct:]?\\d{4}"
str_detect(y, regex_phonenumber)
regex_phonenumber = "\\d{3}\\?[:punct:]?\\d{3}\?\[:punct:]?\\d{4}"
regex_phonenumber = "\\d{3}\\?[:punct:]?\\d{3}\\?[:punct:]?\\d{4}"
str_detect(y, regex_phonenumber)
regex_phonenumber = "\\d{3}\\d{3}\\d{4}"
str_detect(y, regex_phonenumber)
regex_phonenumber = "\\d{3}\\[:punct:]\\d{3}\\[:punct:]\\d{4}"
str_detect(y, regex_phonenumber)
regex_phonenumber = "[:punct:]?\\d{3}[:punct:]? ?\\d{3}[:punct:]\\d{4}"
str_detect(y, regex_phonenumber)
regex_phonenumber = "[:punct:]?\\d{3}[:punct:]? ?\\d{3}[:punct:]?\\d{4}"
str_detect(y, regex_phonenumber)
nstall.packages("tidytext")
install.packages("wordcloud")
install.packages("sentimentr")
install.packages(c("igraph", "ggforce", "ggraph"))
install.packages("topicmodels")
install.packages("broom")
install.packages("tictoc")
install.packages("spacyr")
library(stringr)
library(tidyverse)
install.packages("broom")
#Natural Language Processing
install.packages("tidytext")
getwd()
folder = "/Users/haonguyen/Desktop/CIS8393 - Using R/aclImdb"
fnames = list.files(folder, recursive = T) #get all filenames under aclImdb
#we are only interested in the txt files under the neg/pos folder
fnames=fnames[str_detect(fnames, "/(neg|pos)/.+txt")]
ll = length(fnames)
ll
install.packages(c("foreach", "doParallel"))
library(foreach)
library(doParallel)
library(purrr)
library(foreach)
library(doParallel)
n_core = parallel::detectCores()
registerDoParallel(n_core) #initiate a parallel cluster
# read files into R in parallel
txts = foreach(i = 1:length(fnames),
.combine = c, .packages = "tidyverse") %dopar% {
read_file(str_c(folder, fnames[i]))
}
walk(fnames[1:6], print)
walk(fnames[(ll-5):ll], print)
#we are only interested in the txt files under the neg/pos folder
fnames=fnames[str_detect(fnames, "(neg|pos)/.+txt")]
n_core = parallel::detectCores()
registerDoParallel(n_core) #initiate a parallel cluster
# read files into R in parallel
txts = foreach(i = 1:length(fnames),
.combine = c, .packages = "tidyverse") %dopar% {
read_file(str_c(folder, fnames[i]))
}
stopImplicitCluster() # stop the cluster if you don't need it anymore
walk(fnames[1:6], print)
walk(fnames[(ll-5):ll], print)
n_core = parallel::detectCores()
registerDoParallel(n_core) #initiate a parallel cluster
# read files into R in parallel
txts = foreach(i = 1:length(fnames),
.combine = c, .packages = "tidyverse") %dopar% {
read_file(str_c(folder, fnames[i]))
}
folder = "/Users/haonguyen/Desktop/CIS8393 - Using R/aclImdb/test"
#we are only interested in the txt files under the neg/pos folder
fnames=fnames[str_detect(fnames, "(neg|pos)/.+txt")]
ll = length(fnames)
ll
walk(fnames[1:6], print)
walk(fnames[(ll-5):ll], print)
n_core = parallel::detectCores()
registerDoParallel(n_core) #initiate a parallel cluster
# read files into R in parallel
txts = foreach(i = 1:length(fnames),
.combine = c, .packages = "tidyverse") %dopar% {
read_file(str_c(folder, fnames[i]))
}
stopImplicitCluster()
folder = "/Users/haonguyen/Desktop/CIS8393 - Using R/aclImdb/"
fnames = list.files(folder, recursive = T) #get all filenames under aclImdb
#we are only interested in the txt files under the neg/pos folder
fnames=fnames[str_detect(fnames, "(neg|pos)/.+txt")]
ll = length(fnames)
ll
walk(fnames[1:6], print)
walk(fnames[(ll-5):ll], print)
install.packages(c("foreach", "doParallel"))
library(purrr)
library(foreach)
library(doParallel)
n_core = parallel::detectCores()
registerDoParallel(n_core) #initiate a parallel cluster
# read files into R in parallel
txts = foreach(i = 1:length(fnames),
.combine = c, .packages = "tidyverse") %dopar% {
read_file(str_c(folder, fnames[i]))
}
stopImplicitCluster() #
install.packages(c("foreach", "doParallel"))
setwd("/Users/haonguyen/Desktop/CIS8393 - Using R/")
folder = "aclImdb/"
fnames = list.files(folder, recursive = T) #get all filenames under aclImdb
#we are only interested in the txt files under the neg/pos folder
fnames=fnames[str_detect(fnames, "(neg|pos)/.+txt")]
ll = length(fnames)
ll
walk(fnames[1:6], print)
n_core = parallel::detectCores()
registerDoParallel(n_core) #initiate a parallel cluster
# read files into R in parallel
txts = foreach(i = 1:length(fnames),
.combine = c, .packages = "tidyverse") %dopar% {
read_file(str_c(folder, fnames[i]))
}
stopImplicitCluster()
df = df %>%
separate(fname,
into=c("type", "polarity", "review_id", "rating", "ext"),
sep="/|_|\\.") %>%
arrange(review_id)
df
df = df %>%
separate(fname,
into=c("type", "polarity", "review_id", "rating", "ext"),
sep="/|_|\\.") %>%
arrange(review_id)
#we are only interested in the txt files under the neg/pos folder
fnames=fnames[str_detect(fnames, "(neg|pos)/.+txt")]
df = df %>%
separate(fname,
into=c("type", "polarity", "review_id", "rating", "ext"),
sep="/|_|\\.") %>%
arrange(review_id)
library(purrr)
library(foreach)
library(doParallel)
n_core = parallel::detectCores()
registerDoParallel(n_core) #initiate a parallel cluster
# read files into R in parallel
txts = foreach(i = 1:length(fnames),
.combine = c, .packages = "tidyverse") %dopar% {
read_file(str_c(folder, fnames[i]))
}
library(tidytext)
library(stringr)
library(tidyverse)
setwd("/Users/haonguyen/Desktop/CIS8393 - Using R/")
folder = "aclImdb/"
fnames = list.files(folder, recursive = T) #get all filenames under aclImdb
#we are only interested in the txt files under the neg/pos folder
fnames=fnames[str_detect(fnames, "(neg|pos)/.+txt")]
ll = length(fnames)
ll
walk(fnames[1:6], print)
walk(fnames[(ll-5):ll], print)
#install.packages(c("foreach", "doParallel"))
library(purrr)
library(foreach)
library(doParallel)
n_core = parallel::detectCores()
registerDoParallel(n_core) #initiate a parallel cluster
# read files into R in parallel
txts = foreach(i = 1:length(fnames),
.combine = c, .packages = "tidyverse") %dopar% {
read_file(str_c(folder, fnames[i]))
}
stopImplicitCluster() # stop the cluster if you don't need it anymore
df = df %>%
separate(fname,
into=c("type", "polarity", "review_id", "rating", "ext"),
sep="/|_|\\.") %>%
arrange(review_id)
df
df = df %>%
separate(fname,
into=c("type", "polarity", "review_id", "rating", "ext"),
sep="/|_|\\.") %>%
arrange(review_id)
df = data_frame(fname = fnames, text = txts)
df
df = df %>%
separate(fname,
into=c("type", "polarity", "review_id", "rating", "ext"),
sep="/|_|\\.") %>%
arrange(review_id)
df
df = df %>%
mutate(doc_id = str_c(type, polarity, review_id, sep = "_")) %>%
mutate(text = str_replace_all(text, "(<br />)+", " "),
review_id = as.numeric(review_id),
rating = as.numeric(rating)) %>%
select(type, polarity, rating, review_id, doc_id, text)
df
library(tidytext)
tokens <- df %>%
unnest_tokens(output = word, input = text)
tokens
tokens %>%
count(word,
sort = TRUE)
sw = get_stopwords()
sw
cleaned_tokens <- tokens %>%
filter(!word %in% sw$word)
nums <- cleaned_tokens %>%
filter(str_detect(word, "^[0-9]")) %>%
select(word) %>% unique()
nums
cleaned_tokens <- cleaned_tokens %>%
filter(!word %in% nums$word)
length(unique(cleaned_tokens$word))
cleaned_tokens %>%
count(word, sort = T) %>%
rename(word_freq = n) %>%
ggplot(aes(x=word_freq)) +
geom_histogram(aes(y=..count..), color="black", fill="blue", alpha=0.3) +
scale_x_continuous(breaks=c(0:5,10,100,500,10e3),
trans="log1p", expand=c(0,0)) +
scale_y_continuous(breaks=c(0,100,1000,5e3,10e3,5e4,10e4,4e4), expand=c(0,0)) +
theme_bw()
rare <- cleaned_tokens %>%
count(word) %>%
filter(n<10) %>%
select(word) %>%
unique()
rare
cleaned_tokens <- cleaned_tokens %>%
filter(!word %in% rare$word)
length(unique(cleaned_tokens$word)
)
library(wordcloud)
# define a nice color palette
pal <- brewer.pal(8,"Dark2")
# plot the 100 most common words
cleaned_tokens %>%
count(word) %>%
with(wordcloud(word, n, random.order = FALSE, max.words = 100, colors=pal))
get_sentiments("nrc")
get_sentiments("afinn")
library(tidytext)
get_sentiments("nrc")
get_sentiments("afinn")
library(textdata)
install.packages("textdata")
library(textdata)
get_sentiments("nrc")
sent_reviews = cleaned_tokens %>%   left_join(get_sentiments("nrc")) %>%  rename(nrc = sentiment) %>%  left_join(get_sentiments("bing")) %>%  rename(bing = sentiment) %>%  left_join(get_sentiments("afinn")) %>%  rename(afinn = value)sent_reviews
sent_reviews = cleaned_tokens %>%   left_join(get_sentiments("nrc")) %>%  rename(nrc = sentiment) %>%  left_join(get_sentiments("bing")) %>%  rename(bing = sentiment) %>%  left_join(get_sentiments("afinn")) %>%  rename(afinn = value)sent_reviews
sent_reviews = cleaned_tokens %>%
left_join(get_sentiments("nrc")) %>%
rename(nrc = sentiment) %>%
left_join(get_sentiments("bing")) %>%
rename(bing = sentiment) %>%
left_join(get_sentiments("afinn")) %>%
rename(afinn = value)
sent_reviews = cleaned_tokens %>%
left_join(get_sentiments("nrc")) %>%
rename(nrc = sentiment) %>%
left_join(get_sentiments("bing")) %>%
rename(bing = sentiment) %>%
left_join(get_sentiments("afinn")) %>%
rename(afinn = value)
sent_reviews
bing_word_counts <- sent_reviews %>%
filter(!is.na(bing)) %>%
count(word, bing, sort = TRUE)
bing_word_counts
bing_word_counts %>%
filter(n > 10000) %>%
mutate(n = ifelse(bing == "negative", -n, n)) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = bing)) +  geom_col() +  coord_flip() +  labs(y = "Contribution to sentiment")
ncr_word_counts <- sent_reviews %>%
filter(!is.na(ncr)) %>%
count(word, ncr, sort = TRUE)
ncr_word_counts
nrc_word_counts <- sent_reviews %>%
filter(!is.na(nrc)) %>%
count(word, ncr, sort = TRUE)
nrc_word_counts
nrc_word_counts <- sent_reviews %>%
filter(!is.na(nrc)) %>%
count(word, nrc, sort = TRUE)
nrc_word_counts
nrc_word_counts %>%
filter(n > 10000) %>%
mutate(n = ifelse(nrc == "negative", -n, n)) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = nrc)) +  geom_col() +  coord_flip() +  labs(y = "Contribution to sentiment")
nrc_word_counts %>%
filter(!is.na(nrc)) %>%
ggplot(aes(word, n, fill = nrc)) +
geom_col() +
coord_flip() +
labs(y = "Contribution to sentiment")
nrc_word_counts %>%
filter(!is.na(nrc)) %>%
ggplot(aes(word, n, fill = nrc)) +
labs(y = "Contribution to sentiment")
nrc_word_counts %>%
filter(!is.na(nrc)) %>%
ggplot(.,aes(x = nrc)) +
geom_bar()
review_sentences <- df %>%
unnest_tokens(output = sentence, input = text, token = "sentences")
review_sentences
library(sentimentr)
sentiment('I am not very happy. He is very happy')
rm(list = ls())
setwd("/Users/haonguyen/Documents/GitHub/BigDataAnalyticFinalProject")
X_train<-read.table("./UCI HAR Dataset/train/X_train.txt")
y_train<-read.table("./UCI HAR Dataset/train/y_train.txt")
X_test<-read.table("./UCI HAR Dataset/test/X_test.txt")
y_test<-read.table("./UCI HAR Dataset/test/y_test.txt")
